{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGwN7isviGaN",
        "outputId": "42ed9ffa-8e11-48c8-f918-8484d88a0981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20J-qJI1inC5",
        "outputId": "7da43f10-1ff1-4dad-a8aa-96803552a54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['starfish' 'shark' 'fish' 'puffin' 'stingray' 'penguin' 'jellyfish']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.tensorflow/train/_annotations.csv')\n",
        "image_dir = '/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.tensorflow/train/'\n",
        "\n",
        "\n",
        "labels = df['class'].unique()\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s94l1VShpBWB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "\n",
        "class_to_id = {label: i+1 for i, label in enumerate(labels)}\n",
        "\n",
        "\n",
        "\n",
        "class ObjectDetectionDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        # Assuming you have defined labels earlier\n",
        "        self.class_to_id = {label: i + 1 for i, label in enumerate(labels)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels.filename.unique())\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.img_labels.filename.unique()[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Original dimensions\n",
        "        original_width, original_height = image.size\n",
        "\n",
        "        # Filter annotations for the current image\n",
        "        img_boxes = self.img_labels[self.img_labels['filename'] == img_name]\n",
        "\n",
        "        boxes = img_boxes[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "\n",
        "        # Convert class names to class IDs\n",
        "        labels = img_boxes['class'].map(self.class_to_id).astype(int)\n",
        "        labels = torch.as_tensor(labels.to_numpy(), dtype=torch.int64)\n",
        "\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        if self.transform:\n",
        "            image, target = self.transform(image, target)\n",
        "\n",
        "        # Include original image dimensions in the target dictionary\n",
        "        target[\"orig_size\"] = torch.tensor([original_height, original_width])\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for handling None values or batches with varying sizes of targets.\n",
        "    \"\"\"\n",
        "    batch = list(filter(lambda x: x is not None, batch))  # Remove None values\n",
        "    if len(batch) == 0:\n",
        "        return torch.Tensor()\n",
        "\n",
        "    images = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "\n",
        "    images = default_collate(images)  # Use the default collate function here\n",
        "    # Targets don't need collating if they are handled as lists in the model\n",
        "\n",
        "    return images, targets\n",
        "\n",
        "\n",
        "\n",
        "def transform(image, target):\n",
        "    original_width, original_height = image.size\n",
        "    new_width, new_height = 224, 224\n",
        "    scale_x, scale_y = new_width / original_width, new_height / original_height\n",
        "\n",
        "    # Resize image\n",
        "    image = F.resize(image, (new_height, new_width))\n",
        "    image = F.to_tensor(image)\n",
        "\n",
        "    # Scale bounding box coordinates\n",
        "    boxes = target[\"boxes\"]\n",
        "    boxes[:, [0, 2]] *= scale_x  # Scale x coordinates\n",
        "    boxes[:, [1, 3]] *= scale_y  # Scale y coordinates\n",
        "    target[\"boxes\"] = boxes\n",
        "\n",
        "\n",
        "    return image, target\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    \"\"\"Randomly horizontally flips the Image with the probability p (default = 0.5)\"\"\"\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.p:\n",
        "            image = F.hflip(image)\n",
        "            width, _ = image.size\n",
        "            xmin = width - target[\"boxes\"][:, 2]\n",
        "            xmax = width - target[\"boxes\"][:, 0]\n",
        "            target[\"boxes\"][:, 0] = xmin\n",
        "            target[\"boxes\"][:, 2] = xmax\n",
        "        return image, target\n",
        "\n",
        "class Resize(object):\n",
        "    \"\"\"Resize the image and its bounding boxes\"\"\"\n",
        "    def __init__(self, size):\n",
        "        self.size = size  # Expected size format: (width, height)\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        original_width, original_height = image.size\n",
        "        scale_x = self.size[0] / original_width\n",
        "        scale_y = self.size[1] / original_height\n",
        "        image = F.resize(image, self.size)\n",
        "        boxes = target[\"boxes\"]\n",
        "        boxes[:, [0, 2]] *= scale_x\n",
        "        boxes[:, [1, 3]] *= scale_y\n",
        "        target[\"boxes\"] = boxes\n",
        "        return image, target\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert PIL Image and target into PyTorch Tensors.\"\"\"\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        return image, target\n",
        "\n",
        "class ColorJitter(transforms.ColorJitter):\n",
        "    \"\"\"Randomly change the brightness, contrast, saturation, and hue of an image.\"\"\"\n",
        "    def __call__(self, image, target):\n",
        "        image = super(ColorJitter, self).__call__(image)\n",
        "        return image, target\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(Resize((224, 224)))  # Resize the image\n",
        "    if train:\n",
        "        transforms.append(RandomHorizontalFlip())  # Randomly flip\n",
        "        transforms.append(ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1))  # Color Jitter\n",
        "    transforms.append(ToTensor())  # Convert to tensor\n",
        "    return Compose(transforms)\n",
        "\n",
        "class Compose(object):\n",
        "    \"\"\"Composes several transforms together.\"\"\"\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n"
      ],
      "metadata": {
        "id": "ZSC-Izdl_57k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9KCQVoTBzv2"
      },
      "outputs": [],
      "source": [
        "train_dataset = ObjectDetectionDataset(\n",
        "    annotations_file='/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.tensorflow/train/_annotations.csv',\n",
        "    img_dir='/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.tensorflow/train',\n",
        "    transform=get_transform(train=True)  # Apply data augmentations for training\n",
        ")\n",
        "\n",
        "val_dataset = ObjectDetectionDataset(\n",
        "    annotations_file='/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.tensorflow/valid/_annotations.csv',\n",
        "    img_dir='/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.tensorflow/valid',\n",
        "    transform=get_transform(train=False)  # Apply only the necessary transformations for validation\n",
        ")\n",
        "\n",
        "test_dataset = ObjectDetectionDataset(\n",
        "    annotations_file='/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.tensorflow/test/_annotations.csv',\n",
        "    img_dir='/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.tensorflow/test',\n",
        "    transform=get_transform(train=False)  # Apply only the necessary transformations for testing\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF4D1vuzyQoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d2fc38-eb0b-4c7b-a697-a67b43b5c138"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "      (2): Hardswish()\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
              "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
              "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
              "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (16): Conv2dNormActivation(\n",
              "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "      (2): Hardswish()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(960, 15, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(960, 60, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=47040, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=8, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=32, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import mobilenet_backbone\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
        "\n",
        "def create_mobilenet_v3_backbone(pretrained=True, trainable_layers=3):\n",
        "    # Specify pretrained weights\n",
        "    weights = MobileNet_V3_Large_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "\n",
        "    # Load the MobileNetV3 model\n",
        "    if weights is not None:\n",
        "        model = mobilenet_v3_large(weights=weights)\n",
        "    else:\n",
        "        model = mobilenet_v3_large(weights=None)\n",
        "\n",
        "    # Extract the backbone\n",
        "    backbone = model.features\n",
        "    backbone.out_channels = 960  # Number of output channels for MobileNetV3's backbone\n",
        "\n",
        "    # Freeze the desired layers\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for layer in model.features[-trainable_layers:]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    return backbone\n",
        "\n",
        "\n",
        "\n",
        "def get_object_detection_model(num_classes):\n",
        "    # Load the MobileNetV3 backbone\n",
        "    backbone = create_mobilenet_v3_backbone(pretrained=True)\n",
        "\n",
        "    # Define the anchor generator for the RPN (Region Proposal Network)\n",
        "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "    # Define the RoI (Region of Interest) pooler for the detection heads\n",
        "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
        "\n",
        "    # Initialize the Faster R-CNN model\n",
        "    model = FasterRCNN(backbone,\n",
        "                       num_classes=num_classes,\n",
        "                       rpn_anchor_generator=anchor_generator,\n",
        "                       box_roi_pool=roi_pooler)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Define the number of classes: 7 classes + background\n",
        "num_classes = 8\n",
        "\n",
        "# Initialize the model\n",
        "model = get_object_detection_model(num_classes)\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define your optimizer, learning rate scheduler, and other training details here\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def iou(boxA, boxB):\n",
        "    # Determine the (x, y)-coordinates of the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    # Compute the area of intersection rectangle\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "\n",
        "    # Compute the area of both the prediction and ground-truth rectangles\n",
        "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "\n",
        "    # Compute the intersection over union by taking the intersection area and dividing it by the sum of prediction + ground-truth areas - the intersection area\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "    # Return the intersection over union value\n",
        "    return iou\n",
        "\n",
        "def calculate_precision(val_data_loader, model, device, iou_threshold=0.5, confidence_threshold=0.5):\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    total_ground_truth = 0\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in val_data_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            outputs = model(images)\n",
        "            for i, output in enumerate(outputs):\n",
        "                pred_boxes = output['boxes'].cpu().numpy()\n",
        "                pred_scores = output['scores'].cpu().numpy()\n",
        "                pred_labels = output['labels'].cpu().numpy()  # Extract predicted labels\n",
        "                high_confidence_mask = pred_scores > confidence_threshold\n",
        "                pred_boxes = pred_boxes[high_confidence_mask]\n",
        "                pred_scores = pred_scores[high_confidence_mask]\n",
        "                pred_labels = pred_labels[high_confidence_mask]  # Apply mask to labels as well\n",
        "                # Apply NMS\n",
        "                keep_indices = non_max_suppression(pred_boxes, pred_scores, 0.5)\n",
        "                pred_boxes = pred_boxes[keep_indices]\n",
        "                pred_scores = pred_scores[keep_indices]\n",
        "                pred_labels = pred_labels[keep_indices]  # Keep labels consistent with boxes\n",
        "                gt_boxes = targets[i]['boxes'].cpu().numpy()\n",
        "                gt_labels = targets[i]['labels'].cpu().numpy()  # Extract ground truth labels\n",
        "                matched_gt_boxes = []\n",
        "                for j, pred_box in enumerate(pred_boxes):\n",
        "                    pred_label = pred_labels[j]  # Get the label for the current predicted box\n",
        "                    best_iou = 0\n",
        "                    best_gt_idx = -1\n",
        "                    for gt_idx, gt_box in enumerate(gt_boxes):\n",
        "                        gt_label = gt_labels[gt_idx]  # Get the label for the current ground truth box\n",
        "                        if gt_idx not in matched_gt_boxes and pred_label == gt_label:  # Check for label match\n",
        "                            current_iou = iou(pred_box, gt_box)\n",
        "                            if current_iou > best_iou:\n",
        "                                best_iou = current_iou\n",
        "                                best_gt_idx = gt_idx\n",
        "                    if best_iou >= iou_threshold:\n",
        "                        true_positives += 1\n",
        "                        matched_gt_boxes.append(best_gt_idx)\n",
        "                    else:\n",
        "                        false_positives += 1\n",
        "                total_ground_truth += len(gt_boxes)\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / total_ground_truth if total_ground_truth > 0 else 0\n",
        "\n",
        "    return precision, recall\n"
      ],
      "metadata": {
        "id": "3OlhXWkojxME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def filter_boxes(boxes, scores, confidence_threshold):\n",
        "    \"\"\"\n",
        "    Filter out boxes with a confidence score below the threshold.\n",
        "\n",
        "    Args:\n",
        "    - boxes (list of lists): List of bounding boxes.\n",
        "    - scores (list): List of confidence scores for each bounding box.\n",
        "    - confidence_threshold (float): Confidence threshold.\n",
        "\n",
        "    Returns:\n",
        "    - Filtered list of boxes and scores.\n",
        "    \"\"\"\n",
        "    filtered_boxes = []\n",
        "    filtered_scores = []\n",
        "    for box, score in zip(boxes, scores):\n",
        "        if score >= confidence_threshold:\n",
        "            filtered_boxes.append(box)\n",
        "            filtered_scores.append(score)\n",
        "    return filtered_boxes, filtered_scores\n",
        "\n",
        "def non_max_suppression(boxes, scores, threshold):\n",
        "    \"\"\"\n",
        "    Perform non-maximum suppression.\n",
        "\n",
        "    Args:\n",
        "    - boxes (list of lists): List of bounding boxes (each box is a list of [x1, y1, x2, y2]).\n",
        "    - scores (list): List of confidence scores for each bounding box.\n",
        "    - threshold (float): Overlap threshold for suppressing redundant boxes.\n",
        "\n",
        "    Returns:\n",
        "    - List of indices corresponding to the boxes that have been kept.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to a numpy array for vectorized operations\n",
        "    boxes = np.array(boxes)\n",
        "    scores = np.array(scores)\n",
        "\n",
        "    # Compute the area of the bounding boxes and sort by the bottom-right y-coordinate of the bounding box\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    order = scores.argsort()[::-1]\n",
        "\n",
        "    keep = []  # Indices of boxes that will be kept\n",
        "    while order.size > 0:\n",
        "        i = order[0]  # Index of the current box with the highest score\n",
        "        keep.append(i)\n",
        "\n",
        "        # Compute overlap between the current box and the rest\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "\n",
        "        w = np.maximum(0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0, yy2 - yy1 + 1)\n",
        "        overlap = (w * h) / areas[order[1:]]  # Intersection over area of other boxes\n",
        "\n",
        "        # Keep boxes with an overlap less than the threshold\n",
        "        inds = np.where(overlap <= threshold)[0]\n",
        "        order = order[inds + 1]\n",
        "\n",
        "    return keep"
      ],
      "metadata": {
        "id": "7fC9ULg2koEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import itertools\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs\n",
        "\n",
        "# Define your hyperparameters space\n",
        "hyperparams = {\n",
        "    'learning_rate': [0.001, 0.0005, 0.0001],\n",
        "    'num_epochs': [5, 10, 15],\n",
        "    'batch_size': [4, 8, 16]\n",
        "}\n",
        "\n",
        "# Initialize the TensorBoard writer\n",
        "writer = SummaryWriter('/content/drive/My Drive/TensorBoard/hparam_tuning')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "for lr, epochs, batch_size in itertools.product(hyperparams['learning_rate'], hyperparams['num_epochs'], hyperparams['batch_size']):\n",
        "    print(f\"\\nTraining with learning rate: {lr}, epochs: {epochs}, batch size: {batch_size}\")\n",
        "\n",
        "    model = get_object_detection_model(8)  # Initialize your model\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0005)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        total_batches = len(train_data_loader)\n",
        "\n",
        "        for batch_index, (images, targets) in enumerate(train_data_loader, start=1):\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            optimizer.zero_grad()\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += losses.item()\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_index}/{total_batches}], Loss: {losses.item():.4f}\")\n",
        "\n",
        "        epoch_loss = running_loss / total_batches\n",
        "        writer.add_scalar(f'Loss/train_lr_{lr}_bs_{batch_size}', epoch_loss, epoch)\n",
        "\n",
        "        val_precision, val_recall = calculate_precision(val_data_loader, model, device, 0.1, 0.5)\n",
        "        writer.add_scalar(f'Precision/val_lr_{lr}_bs_{batch_size}', val_precision, epoch)\n",
        "        writer.add_scalar(f'Recall/val_lr_{lr}_bs_{batch_size}', val_recall, epoch)\n",
        "\n",
        "        print(f\"End of Epoch {epoch+1}/{epochs}, Average Loss: {epoch_loss:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n",
        "\n",
        "    # Log hyperparameters and their final performance metrics\n",
        "    writer.add_hparams({'lr': lr, 'epochs': epochs, 'batch_size': batch_size},\n",
        "                       {'hparam/precision': val_precision, 'hparam/recall': val_recall})\n",
        "\n",
        "    print(f\"Completed training with lr={lr}, epochs={epochs}, batch_size={batch_size}. Final Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\\n\")\n",
        "\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "34FrQZlDknkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import sys\n",
        "\n",
        "# Initialize TensorBoard writer\n",
        "writer = SummaryWriter('/content/drive/My Drive/TensorBoard/final_run')\n",
        "\n",
        "# Set hyperparameters based on tuning results\n",
        "batch_size = 4\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 15\n",
        "\n",
        "# Prepare the data loader\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize the model\n",
        "model = get_object_detection_model(8)  # Assuming 8 classes + background\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available. Using GPU...\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available. Using CPU...\")\n",
        "model.to(device)\n",
        "\n",
        "# Set the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0005)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_batches = len(train_data_loader)\n",
        "    for batch_index, (images, targets) in enumerate(train_data_loader, start=1):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress update\n",
        "        sys.stdout.write(f'\\rEpoch {epoch+1}/{num_epochs}, Batch {batch_index}/{total_batches}, Loss: {losses.item():.4f}')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    # Validation step - evaluate precision and recall after each epoch\n",
        "    precision, recall = calculate_precision(val_data_loader, model, device, iou_threshold=0.1, confidence_threshold=0.5)\n",
        "\n",
        "    # Log precision and recall to TensorBoard\n",
        "    writer.add_scalar('Precision/validation', precision, epoch)\n",
        "    writer.add_scalar('Recall/validation', recall, epoch)\n",
        "\n",
        "    print(f'\\nEpoch {epoch+1}/{num_epochs}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
        "    if epoch == 14:  # Save model at the 15th epoch\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/ModelWeights')\n",
        "        print(\"Saved model weights at epoch 15\")\n",
        "\n",
        "# Close the writer when done\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "yzpjNe5AgaYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a824862-fc55-440c-a035-9b9aafa24b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available. Using GPU...\n",
            "Epoch 1/15, Batch 112/112, Loss: 0.4887\n",
            "Epoch 1/15, Precision: 0.9167, Recall: 0.0121\n",
            "Epoch 2/15, Batch 112/112, Loss: 0.3496\n",
            "Epoch 2/15, Precision: 0.9189, Recall: 0.0374\n",
            "Epoch 3/15, Batch 112/112, Loss: 0.8015\n",
            "Epoch 3/15, Precision: 0.8657, Recall: 0.2057\n",
            "Epoch 4/15, Batch 112/112, Loss: 0.4501\n",
            "Epoch 4/15, Precision: 0.8315, Recall: 0.2552\n",
            "Epoch 5/15, Batch 112/112, Loss: 0.3679\n",
            "Epoch 5/15, Precision: 0.8185, Recall: 0.2926\n",
            "Epoch 6/15, Batch 112/112, Loss: 0.2776\n",
            "Epoch 6/15, Precision: 0.7862, Recall: 0.4246\n",
            "Epoch 7/15, Batch 112/112, Loss: 0.2658\n",
            "Epoch 7/15, Precision: 0.8465, Recall: 0.4246\n",
            "Epoch 8/15, Batch 112/112, Loss: 0.6420\n",
            "Epoch 8/15, Precision: 0.8440, Recall: 0.4048\n",
            "Epoch 9/15, Batch 112/112, Loss: 0.7402\n",
            "Epoch 9/15, Precision: 0.8223, Recall: 0.4631\n",
            "Epoch 10/15, Batch 112/112, Loss: 0.9006\n",
            "Epoch 10/15, Precision: 0.7803, Recall: 0.5314\n",
            "Epoch 11/15, Batch 112/112, Loss: 0.2207\n",
            "Epoch 11/15, Precision: 0.7992, Recall: 0.4686\n",
            "Epoch 12/15, Batch 112/112, Loss: 0.3316\n",
            "Epoch 12/15, Precision: 0.7702, Recall: 0.5347\n",
            "Epoch 13/15, Batch 112/112, Loss: 0.5839\n",
            "Epoch 13/15, Precision: 0.8390, Recall: 0.5160\n",
            "Epoch 14/15, Batch 112/112, Loss: 0.4144\n",
            "Epoch 14/15, Precision: 0.7868, Recall: 0.5644\n",
            "Epoch 15/15, Batch 112/112, Loss: 0.4222\n",
            "Epoch 15/15, Precision: 0.7720, Recall: 0.5215\n",
            "Saved model weights at epoch 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tufUv5hpsfm"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import functional as F\n",
        "\n",
        "def val_transform(image, target):\n",
        "    # Example transformation: Resize and convert to tensor\n",
        "    image = F.resize(image, (224, 224))\n",
        "    image = F.to_tensor(image)\n",
        "    # Apply similar transformations to target if necessary\n",
        "    return image, target\n",
        "\n",
        "\n",
        "# Usage example\n",
        "val_dataset = ObjectDetectionDataset(\n",
        "    annotations_file='/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.tensorflow/valid/_annotations.csv',\n",
        "    img_dir='/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.tensorflow/valid/',\n",
        "     transform=get_transform(train=False)\n",
        ")\n",
        "\n",
        "# Initialize the DataLoader for the validation dataset\n",
        "val_data_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=64,  # Adjust as per your GPU memory\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn  # Your custom collate function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SU1AVJTKvkBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMyb726cGL94"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "id_to_class = {v: k for k, v in class_to_id.items()}\n",
        "def visualize_with_boxes(image, boxes, labels, scores):\n",
        "    \"\"\"\n",
        "    Visualize the image with bounding boxes and labels.\n",
        "\n",
        "    Args:\n",
        "    - image (tensor): The image tensor.\n",
        "    - boxes (array): The bounding boxes array.\n",
        "    - labels (array): The labels array corresponding to the boxes.\n",
        "    - scores (array): The confidence scores for each box.\n",
        "    \"\"\"\n",
        "    # Convert image tensor to numpy for visualization\n",
        "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(image_np)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        # Create a Rectangle patch\n",
        "        rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
        "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Add label and score text\n",
        "        plt.text(box[0], box[1], f'{id_to_class[label]}: {score:.2f}',\n",
        "                 bbox=dict(facecolor='yellow', alpha=0.5), fontsize=8, color='black')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# # Set model to evaluation mode and disable gradients\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Process a single batch from the validation data loader\n",
        "images, _ = next(iter(val_data_loader))  # Get the next batch of images and targets\n",
        "images = [img.to(device) for img in images]  # Move images to the appropriate device\n",
        "\n",
        "outputs = model(images)  # Get predictions from the model\n",
        "outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]  # Move everything to CPU\n",
        "confidence_threshold = 0.5\n",
        "# For each image in the batch\n",
        "for i in range(len(images)):\n",
        "    # Extract boxes, labels, and scores\n",
        "    pred_boxes = outputs[i]['boxes'].numpy()\n",
        "    pred_scores = outputs[i]['scores'].numpy()\n",
        "    pred_labels = outputs[i]['labels'].numpy()\n",
        "    print(pred_scores)\n",
        "    high_confidence_mask = pred_scores > confidence_threshold\n",
        "    pred_boxes = pred_boxes[high_confidence_mask]\n",
        "    pred_scores = pred_scores[high_confidence_mask]\n",
        "\n",
        "\n",
        "\n",
        "    # Apply NMS\n",
        "    keep_indices = non_max_suppression(pred_boxes, pred_scores, 0.3)\n",
        "    final_boxes = pred_boxes[keep_indices]\n",
        "    final_labels = pred_labels[keep_indices]\n",
        "    final_scores = pred_scores[keep_indices]\n",
        "\n",
        "    # Visualize predictions for each image\n",
        "    visualize_with_boxes(images[i], final_boxes, final_labels, final_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9wTVTraEnYq",
        "outputId": "b3bccdf4-f760-4e85-dc14-03201a9076cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7140804597701149\n",
            "Recall: 0.5467546754675467\n"
          ]
        }
      ],
      "source": [
        "# Call the function\n",
        "precision, recall = calculate_precision(val_data_loader, model, device, 0.2, 0.5)\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pAaQfRH8g48",
        "outputId": "d056b91e-f67e-4f77-f895-f88de56c5275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.760806916426513\n",
            "Recall: 0.4520547945205479\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Call the function\n",
        "precision, recall = calculate_precision(test_data_loader, model, device, 0.2, 0.5)\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC1zoG-Hl-IV"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "# Mount Google Drive\n",
        "\n",
        "\n",
        "# Path to your MP4 file in Google Drive\n",
        "mp4_path = '/content/drive/My Drive/CoralReef.mp4'\n",
        "id_to_class = {v: k for k, v in class_to_id.items()}\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Assume 'model' is your object detection model and it's already loaded and moved to the correct device (CPU or GPU)\n",
        "\n",
        "# Function to preprocess input images\n",
        "def preprocess_image(frame):\n",
        "    # Convert the BGR image to RGB, then convert to PIL Image\n",
        "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Transform the image as required by your model (e.g., resizing, normalization)\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize((224, 224)),  # Example resize, adjust to your model's input size\n",
        "        torchvision.transforms.ToTensor(),\n",
        "    ])\n",
        "    img = transform(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "def draw_boxes(frame, boxes, labels, scores, threshold=0.5):\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        if score > threshold:\n",
        "            # Convert box coordinates to integers\n",
        "            x1, y1, x2, y2 = box.int().tolist()  # Converts to int and then to a list of Python integers\n",
        "\n",
        "            # Apply scale factors to adjust the box size back to the frame's dimensions\n",
        "            x1 = int(x1 * scale_x)\n",
        "            y1 = int(y1 * scale_y)\n",
        "            x2 = int(x2 * scale_x)\n",
        "            y2 = int(y2 * scale_y)\n",
        "\n",
        "            # Draw rectangles and text on the frame\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Ensure the label is a string if you want to display it\n",
        "            label_str = str(label) if isinstance(label, int) else label\n",
        "            cv2.putText(frame, f'{id_to_class[label_str.item()]}: {score:.2f}', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
        "    return frame\n",
        "\n",
        "# Correct place to define VideoWriter, after obtaining original frame dimensions\n",
        "cap = cv2.VideoCapture(mp4_path)\n",
        "ret, frame = cap.read()  # Read once to get the original size\n",
        "if ret:\n",
        "    original_height, original_width = frame.shape[:2]\n",
        "\n",
        "# Now define VideoWriter with the correct frame size\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V')  # Define the codec for .mp4 files\n",
        "out = cv2.VideoWriter('/content/drive/My Drive/CoralReefUpdated2.mp4', fourcc, 20.0, (original_width, original_height))\n",
        "\n",
        "# If the first read was successful, rewind or re-initialize the capture to start from the first frame again\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break  # Exit loop if no more frames to read\n",
        "\n",
        "# Calculate scale factors\n",
        "    scale_x = original_width / 224\n",
        "    scale_y = original_height / 224\n",
        "    if not ret:\n",
        "        break  # Break the loop if there are no frames left to read\n",
        "\n",
        "    # Preprocess the frame\n",
        "    img = preprocess_image(frame)\n",
        "    img = img.unsqueeze(0)  # Add batch dimension\n",
        "    img = img.to(device)  # Move to the same device as your model\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        predictions = model(img)\n",
        "\n",
        "\n",
        "    # Assuming 'predictions' is the output from your model\n",
        "\n",
        "# Ensure pred_boxes and pred_scores are tensors and moved to the correct device\n",
        "    pred_boxes = predictions[0]['boxes'].to(device)  # Ensure it's on the same device as model\n",
        "    pred_scores = predictions[0]['scores'].to(device)\n",
        "\n",
        "# Convert boxes and scores to float32 if not already\n",
        "    pred_boxes = pred_boxes.float()\n",
        "    pred_scores = pred_scores.float()\n",
        "\n",
        "# Apply NMS\n",
        "    indices = torchvision.ops.nms(pred_boxes, pred_scores, 0.1)  # Adjust threshold as needed\n",
        "\n",
        "# Use indices to select boxes, labels, and scores\n",
        "    nms_boxes = pred_boxes[indices]\n",
        "    nms_labels = predictions[0]['labels'][indices].to(device)  # Adjusting for device as well\n",
        "    nms_scores = pred_scores[indices]\n",
        "\n",
        "\n",
        "    # Draw bounding boxes on the original frame\n",
        "    frame = draw_boxes(frame, nms_boxes, nms_labels, nms_scores, threshold=0.5)\n",
        "    out.write(frame)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Press 'q' to exit early\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release everything if job is finished\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}